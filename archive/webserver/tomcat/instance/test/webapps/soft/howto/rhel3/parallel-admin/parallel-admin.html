<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN">
<HTML
><HEAD
><TITLE
>Administration of SPK for Parallel Computation</TITLE
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.76b+
"></HEAD
><BODY
CLASS="article"
BGCOLOR="#FFFFFF"
TEXT="#000000"
LINK="#0000FF"
VLINK="#840084"
ALINK="#0000FF"
><DIV
CLASS="ARTICLE"
><DIV
CLASS="TITLEPAGE"
><H1
CLASS="title"
><A
NAME="AEN1"
></A
>Administration of SPK for Parallel Computation</H1
><DIV
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="80%"
CELLSPACING="10"
CELLPADDING="0"
ALIGN="CENTER"
><TR
><TD
VALIGN="TOP"
><B
>Abstract</B
></TD
></TR
><TR
><TD
VALIGN="TOP"
><P
>&#13;	SPK is designed to be executed on a cluster of computers
	loosely linked together by software known as the Parallel
	Virtual Machine (pvm). This document describes the system
	administration of SPK in the pvm environment. It is a 
	companion to the document
	<A
HREF="../pvm-admin/pvm-admin.html"
TARGET="_top"
>&#13;	  PVM Administration at RFPK
	</A
>
      </P
><P
>&#13;	The information in this document is targeted primarily toward
	the RFPK Software Team and associates and is specific to the
	computer systems and network installed in the RFPK Laboratory
	of the Department of Bioengineering of the University of Washington.  RFPK is the Resource for
	Population Kinetics.  Its work is supported, in part, by grant
	P41 EB-001975 of the National Institutes of Health (NIH) of
	the U.S.  Department of Health and Human Services.
      </P
><P
>&#13;	<A
HREF="#AEN238"
>Copyright</A
> (c) 2005, by the University of Washington.
      </P
></TD
></TR
></TABLE
></DIV
><DIV
CLASS="revhistory"
><TABLE
WIDTH="100%"
BORDER="0"
><TR
><TH
ALIGN="LEFT"
VALIGN="TOP"
COLSPAN="3"
><B
>Revision History</B
></TH
></TR
><TR
><TD
ALIGN="LEFT"
>Revision 1.0</TD
><TD
ALIGN="LEFT"
>February 9, 2005</TD
><TD
ALIGN="LEFT"
>Revised by: afw</TD
></TR
><TR
><TD
ALIGN="LEFT"
COLSPAN="3"
>Initial version.</TD
></TR
></TABLE
></DIV
><HR
WIDTH="75%"
ALIGN="CENTER"
COLOR="#000000"
SIZE="1"></DIV
><DIV
CLASS="TOC"
><DL
><DT
><B
>Table of Contents</B
></DT
><DT
><A
HREF="#AEN16"
>Overview</A
></DT
><DT
><A
HREF="#AEN81"
>Group Configuration for Aspkserver and All PVM Nodes</A
></DT
><DT
><A
HREF="#AEN85"
>Additional Group Configuration to Selected Nodes</A
></DT
><DT
><A
HREF="#AEN89"
>NFS Configuration of aspkserver</A
></DT
><DT
><A
HREF="#AEN111"
>NFS Configuration of the PVM Nodes</A
></DT
><DT
><A
HREF="#AEN140"
>Environment Variables and File Creation Mask</A
></DT
><DT
><A
HREF="#AEN145"
>Building and Installing the Prototype</A
></DT
><DD
><DL
><DT
><A
HREF="#AEN148"
>LINUXI386 architecture</A
></DT
><DT
><A
HREF="#AEN173"
>LINUXX86_64 architecture</A
></DT
></DL
></DD
><DT
><A
HREF="#AEN203"
>Running the Parallel SPK Prototype</A
></DT
><DT
><A
HREF="#AEN238"
>Copyright Notice</A
></DT
></DL
></DIV
><DIV
CLASS="sect1"
><H2
CLASS="sect1"
><A
NAME="AEN16"
></A
>Overview</H2
><P
>&#13;      The principal unit of work in SPK is called a job.  It is created by
      a user when, with the help of a graphical tool called the MDA, she
      combines a model with data and various parameters and submits it
      via the internet for processing.
    </P
><P
>&#13;      Before the job can begin its 
      computational phase, it first travels to a server known as
      the Application Server for Population Kinetics
      <I
CLASS="emphasis"
>(aspkserver)</I
>, where it is compiled 
      by the ASPK compiler to create three C++ programs, called 
      <I
CLASS="emphasis"
>spkjob</I
>, 
      <I
CLASS="emphasis"
>spkpop</I
> and <I
CLASS="emphasis"
>spkind</I
>.
      The job is then queued for execution.
    </P
><P
>&#13;      The next step in the history of the job is to be selected for
      processing on a group of computers harnessed for 
      parallel computation by software called the Parallel Virtual
      Machine (pvm). We refer to each of these computers as
      a <I
CLASS="emphasis"
>node</I
> or a <I
CLASS="emphasis"
>host</I
>
      and to the collection of computers as the 
      <I
CLASS="emphasis"
>cluster</I
> or the <I
CLASS="emphasis"
>pvm</I
>.
    </P
><P
>&#13;      One node of the pvm is distinguished as the 
      <I
CLASS="emphasis"
>head node</I
>.  In terms of the SPK architecture,
      this is the Computational Server for Population Kinetics
      <I
CLASS="emphasis"
>(cspkserver)</I
>.
    </P
><P
>&#13;      On cspkserver, the three programs output by the ASPK
      compiler are further compiled and linked to the SPK library
      to create an executable binary for spkjob, for spkpop and
      for spkind.
    </P
><P
>&#13;      Spkjob is then executed on cspkserver itself. It utilizes pvm
      functions to spawn spkpop as a subtask.  Which node spkpop actually
      runs on is determined by load distribution algorithms in pvm.
    </P
><P
>&#13;      Spkpop then performs an iterative computation with the goal of
      generating the results specified by the user. With each 
      iteration, spkpop spawns <I
CLASS="emphasis"
>n</I
> instances of
      spkind, where <I
CLASS="emphasis"
>n</I
> is the number of individuals
      in the population being modeled. Which node a particular 
      spkind runs on is determined by pvm.
    </P
><P
>&#13;      Spkjob, spkpop, and the <I
CLASS="emphasis"
>n</I
> instances of 
      spkind all run as independent processes, under the linux or
      unix operating system.  The parent process of spkjob is the
      SPK runtime daemon, which executes continuously on 
      cspkserver.  The parent process of spkpop is a pvm daemon
      running on the node to which it was assigned.  Similarly,
      each instance of spkind is the child of a pvm daemon running
      on its node.  
    </P
><P
>&#13;      On every unix or linux host, parent/child relationships
      create a single process hierarchy.  This hierarchy does not
      extend beyond that host, however, to other hosts in the cluster.
      With pvm, there is a second hierarchy, maintained by 
      communication between daemons, which does span the 
      boundaries between hosts. In pvm parlance the
      programs running in the hierarchy are referred to as
      <I
CLASS="emphasis"
>tasks</I
> to distinguish these
      relationships from those between linux/unix
      <I
CLASS="emphasis"
>processes</I
> and the hierarchy is the
      <I
CLASS="emphasis"
>pvm task hierarchy</I
>.
    </P
><P
>&#13;      SPK uses two channels for inter-task communications:
      <P
></P
><OL
TYPE="1"
><LI
><P
>&#13;	    <I
CLASS="emphasis"
>PVM message passing</I
>, using functions provided by a
	    pvm. This communication channel is used for pvm event messages
	    and application log messages.
	  </P
></LI
><LI
><P
>&#13;	    <I
CLASS="emphasis"
>Shared file hierarchy</I
>. The ubiquitous 
	    Network File System (NFS) works well for this purpose,
	    although nearly any other distributed file system 
	    can be used in its place. This communication channel is used
	    for the communication of application inputs and outputs.
	  </P
></LI
></OL
>
    </P
><P
>&#13;      There is no system administration required for pvm message passing, beyond
      installation and configuration of pvm as described in
      <A
HREF="../pvm-admin/pvm-admin.html"
TARGET="_top"
>&#13;	PVM Administration at RFPK
      </A
>.  Spkjob and spkpop set up the message channels and pvm does the
      rest automatically.
    </P
><P
>&#13;      The shared file hierarchy is created and maintain entirely by linux/unix
      system administration functions.  Here are some of the features of the
      file hierarchy:
      <P
></P
><OL
TYPE="1"
><LI
><P
>&#13;	    The files actually reside on aspkserver, both because of its large
	    disk capacity and its tape backup system.  The aspkserver
	    uses NFS to <I
CLASS="emphasis"
>export</I
> this hierarchy.
	  </P
></LI
><LI
><P
>&#13;	    All the nodes of the pvm <I
CLASS="emphasis"
>mount</I
>
	    the file hierarchy as a filesystem
	    of type <I
CLASS="emphasis"
>nfs</I
>.
	  </P
></LI
><LI
><P
>&#13;	    The directory tree looks like this:
	    <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;	      /usr/local/spk/share/working/spkprod
	      ............................/spktest
	      ..................../log/spkprod
	      ......................../spktest
	      ..................../include/spkprod
	      ............................/spktest
	      ..................../arch/LINUXI386/lib/spkprod
	      ......................................./spktest
	      ........................./LINUXX86_64/lib/spkprod
	      ........................................./spktest
	      ..................../arch/LINUXI386/bin/spkprod
	      ......................................./spktest
	      ........................./LINUXX86_64/bin/spkprod
	      ........................................./spktest
	    </PRE
></TD
></TR
></TABLE
>
	  </P
></LI
><LI
><P
>&#13;	    The purpose of the key directory nodes is as follows:
	    <P
></P
><OL
TYPE="a"
><LI
><P
>&#13;		  <I
CLASS="emphasis"
>working:</I
> the hierarchy of working directories 
		  goes here.  Spkjob creates a directory for the job and spkpop
		  creates a subdirectory for each instance.
		</P
></LI
><LI
><P
>&#13;		  <I
CLASS="emphasis"
>log:</I
> message log files go here.  A message log
		  file consists of a sequence of messages in the order that they
		  were created by spkjob or received by spkjob from spkpop or
		  spkind.  Each message has a header containing a timestamp,
		  job-id number, program name and, in the case of instances of
		  spkind, instance number.
		</P
></LI
><LI
><P
>&#13;		  <I
CLASS="emphasis"
>include:</I
> <TT
CLASS="filename"
>.h</TT
> files to be
		  included with the source generated by the ASPK compiler.
		</P
></LI
><LI
><P
>&#13;		  <I
CLASS="emphasis"
>arch:</I
> binary executables and libraries for each
		  of the processor architectures present in the pvm.
		</P
></LI
></OL
>
	  </P
></LI
></OL
>
    </P
></DIV
><DIV
CLASS="sect1"
><HR><H2
CLASS="sect1"
><A
NAME="AEN81"
></A
>Group Configuration for Aspkserver and All PVM Nodes</H2
><P
>&#13;      The following configuration must be done to all nodes, including the head node,
      as well as for aspkserver.
      As the root user on each node execute the following commands:
      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;/usr/sbin/groupadd -g 444 -r spkshare
/usr/bin/gpasswd -a pvm spkshare
      </PRE
></TD
></TR
></TABLE
>
    </P
></DIV
><DIV
CLASS="sect1"
><HR><H2
CLASS="sect1"
><A
NAME="AEN85"
></A
>Additional Group Configuration to Selected Nodes</H2
><P
>&#13;      It will be convenient to add developers to the spkshare group on 
      aspkserver and cspkserver.  As root, on each of these servers,
      execute the following commands:
      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;/usr/bin/gpasswd -a alan spkshare
/usr/bin/gpasswd -a honda spkshare
/usr/bin/gpasswd -a jiaji spkshare
/usr/bin/gpasswd -a watrous spkshare
      </PRE
></TD
></TR
></TABLE
>/usr/bin/gpasswd -a alan spkshare
    </P
></DIV
><DIV
CLASS="sect1"
><HR><H2
CLASS="sect1"
><A
NAME="AEN89"
></A
>NFS Configuration of aspkserver</H2
><P
>&#13;      In the following, it is assumed that the <I
CLASS="emphasis"
>nfs</I
>
      and <I
CLASS="emphasis"
>portmap</I
> services are running.
    </P
><P
>&#13;      <P
></P
><OL
TYPE="1"
><LI
><P
>&#13;	    <I
CLASS="emphasis"
>&#13;	      Open a terminal window on aspkserver and become root.
	    </I
>
	  </P
></LI
><LI
><P
>&#13;	    <I
CLASS="emphasis"
>Make the directory hierarchy:</I
>
	    <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;mkdir -p /usr/local/spk/share/working/spkprod
mkdir -p /usr/local/spk/share/working/spktest
mkdir -p /usr/local/spk/share/log/spkprod
mkdir -p /usr/local/spk/share/log/spktest
mkdir -p /usr/local/spk/share/include/spkprod
mkdir -p /usr/local/spk/share/include/spktest
mkdir -p /usr/local/spk/share/arch/lib/LINUXI386/spkprod
mkdir -p /usr/local/spk/share/arch/lib/LINUXI386/spktest
mkdir -p /usr/local/spk/share/arch/lib/LINUXX86_64/spkprod
mkdir -p /usr/local/spk/share/arch/lib/LINUXX86_64/spktest
mkdir -p /usr/local/spk/share/arch/bin/LINUXI386/spkprod
mkdir -p /usr/local/spk/share/arch/bin/LINUXI386/spktest
mkdir -p /usr/local/spk/share/arch/bin/LINUXX86_64/spkprod
mkdir -p /usr/local/spk/share/arch/bin/LINUXX86_64/spktest
cd /usr/local/spk/share
chgrp -R spkshare .
chmod -R 2775 .
	    </PRE
></TD
></TR
></TABLE
>
	  </P
></LI
><LI
><A
NAME="exports"
></A
><P
>&#13;	    <I
CLASS="emphasis"
>Export the directory hierarchy.</I
>
	  </P
><P
>&#13;	    Edit the file /etc/exports, appending a line
	    <I
CLASS="emphasis"
>for each node in the pvm</I
>:
	    <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;/usr/local/spk/share   cspk(rw, sync, no_root_squash)
/usr/local/spk/share   jambutty(rw, sync, no_root_squash)
	      .
	      .
	      .
/usr/local/spk/share   rose(rw, sync, no_root_squash)
	    </PRE
></TD
></TR
></TABLE
>
	  </P
><P
>&#13;	    Activate the list by executing the following command:
	    <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;/usr/sbin/exportfs -a
	    </PRE
></TD
></TR
></TABLE
>
	  </P
></LI
></OL
>
    </P
></DIV
><DIV
CLASS="sect1"
><HR><H2
CLASS="sect1"
><A
NAME="AEN111"
></A
>NFS Configuration of the PVM Nodes</H2
><P
>&#13;      Apply the following procedure to each node of the pvm, including
      the head node.
    </P
><P
></P
><OL
TYPE="1"
><LI
><P
>&#13;	  <I
CLASS="emphasis"
>&#13;	    Open a terminal window on the node and become root.
	  </I
>
	</P
></LI
><LI
><P
>&#13;	  <I
CLASS="emphasis"
>Edit the hosts table.</I
>
	  Make sure that there is a line in the 
	  <TT
CLASS="filename"
>/etc/hosts</TT
> file for the IP address of
	  aspkserver.  If such a line is already there, but the 
	  name <I
CLASS="emphasis"
>aspkserver</I
> is not among the 
	  aliases for the address, add it to the line.
	  <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;192.168.1.2      aspkserver
	  </PRE
></TD
></TR
></TABLE
>
	</P
></LI
><LI
><P
>&#13;	  <I
CLASS="emphasis"
>Edit the filesystem table</I
>.  Add the following
	  line to <TT
CLASS="filename"
>/etc/fstab</TT
>:
	  <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;aspkserver:/usr/local/spk/share  /usr/local/spk/share  nfs rsize=8194,wsize=8192,timeo=14,intr
	  </PRE
></TD
></TR
></TABLE
>
	</P
></LI
><LI
><P
>&#13;	  <I
CLASS="emphasis"
>Create a node on which to mount the filesystem:</I
>
	  <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;mkdir -p /usr/local/spk/share
	  </PRE
></TD
></TR
></TABLE
>
	</P
></LI
><LI
><P
>&#13;	  If <I
CLASS="emphasis"
>aspkserver</I
> has not yet been configured to 
	  export the directory hierarchy to this node, 
	  <A
HREF="#exports"
>&#13;	    do it now
	  </A
>.
	</P
></LI
><LI
><P
>&#13;	  Mount the filesystem in order to check your work.  Normally, the
	  filesystem will be mount automatically, each time that the node
	  reboots.
	  <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;mount /usr/local/spk/share
ls -l /usr/local/spk/share
	  </PRE
></TD
></TR
></TABLE
>
	</P
></LI
></OL
></DIV
><DIV
CLASS="sect1"
><HR><H2
CLASS="sect1"
><A
NAME="AEN140"
></A
>Environment Variables and File Creation Mask</H2
><P
>&#13;      As root on cspkserver, add the following lines to the end
      of <TT
CLASS="filename"
>~pvm/.bashrc</TT
>:
      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;SPK_SHARE=/usr/local/spk/share
PVMHOSTFILE=$SPK_SHARE/pvmhosts
export SPK_SHARE PVMHOSTFILE
PATH=$PATH:$SPK_SHARE/arch/bin/$PVM_ARCH/spktest
umask 0002
      </PRE
></TD
></TR
></TABLE
>
    </P
></DIV
><DIV
CLASS="sect1"
><HR><H2
CLASS="sect1"
><A
NAME="AEN145"
></A
>Building and Installing the Prototype</H2
><P
>&#13;      The prototype must be built and installed both for the 
      LINUXI386 architecture and the LINUXX86_64 architecture.
    </P
><DIV
CLASS="sect2"
><HR><H3
CLASS="sect2"
><A
NAME="AEN148"
></A
>LINUXI386 architecture</H3
><P
>&#13;	In the following, all of the steps except the last one are concerned
	with configuration and verification of the configuration.
	The first time through, you should go through all the steps.
	Subsequently, you can just skip to the last step.
      </P
><P
>&#13;	For this procedure to work, you must be logged in under your normal
	ordinary user name to a 
	workstation with LINUXI386 architecture (Intel or AMD 32-bit architecture)
	and must have a cvs workspace.
      </P
><P
>&#13;	<P
></P
><OL
TYPE="1"
><LI
><P
>&#13;	      If the workstation is not already configured to be part of the
	      pvm, follow the instructions in
	      <A
HREF="../pvm-admin/pvm-admin.html"
TARGET="_top"
>&#13;		PVM Administration at RFPK
	      </A
>
	      and in this document to make it so.
	    </P
></LI
><LI
><P
>&#13;	      If the command
	      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;set | egrep '(PVM)|(SPK)'
	      </PRE
></TD
></TR
></TABLE
>
	      shows that PVM_ROOT, PVM_ARCH and SPK_SHARE are all defined,
	      <I
CLASS="emphasis"
>skip to the next step</I
>.
	    </P
><P
>&#13;	      Otherwise, edit <TT
CLASS="filename"
>~/.bashrc</TT
> to add the following lines:
	      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;PVM_ROOT=/usr/share/pvm3
SPK_SHARE=/usr/local/spk/share
export PVM_ROOT SPK_SHARE
PVM_ARCH=$($PVM_ROOT/lib/pvmgetarch)
export PVM_ARCH
	      </PRE
></TD
></TR
></TABLE
>
	      then exit the terminal window and open a new one, in order to 
	      activate the definitions.
	    </P
></LI
><LI
><P
>&#13;	      Make sure that your user name is a member of the <I
CLASS="emphasis"
>spkshare</I
>
	      group.  You can determine this by executing the command
	      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;grep spkshare /etc/group
	      </PRE
></TD
></TR
></TABLE
>
	      and seeing whether or not
	      your user name is in the list at the end of the entry that is displayed.
	      If not, execute the following from a terminal window owned by your ordinary
	      user name:
	      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;su
gpasswd -a $USER spkshare
exit
	      </PRE
></TD
></TR
></TABLE
>
	    </P
></LI
><LI
><P
>&#13;	      In your cvs workspace, go to the directory
	      <TT
CLASS="filename"
>r2/src/misc/parallel</TT
> and execute the following 
	      commands:
	      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;make
make install
	      </PRE
></TD
></TR
></TABLE
>
	    </P
></LI
></OL
>
      </P
></DIV
><DIV
CLASS="sect2"
><HR><H3
CLASS="sect2"
><A
NAME="AEN173"
></A
>LINUXX86_64 architecture</H3
><P
>&#13;	In the following, several steps are concerned with configuration.
	The first time through, you should go through all the steps.
	Subsequently, you can skip the configuration steps.
      </P
><P
>&#13;	<P
></P
><OL
TYPE="1"
><LI
><P
>&#13;	      This process starts on your 32-bit workstation, where it is assumed
	      that your cvs workspace resides, then continues on the 64-bit
	      cspkserver.
	    </P
><P
>&#13;	      Starting in the source code directory,
	      <TT
CLASS="filename"
>r2/src/misc/parallel</TT
>,
	      make a tarball, then copy it to your home directory on cspkserver:
	      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;cd ..                          # go one directory above the source directory
tar cvzf parallel.tgz parallel # make a tarball
scp parallel.tgz cspkserver:   # copy it to cspkserver
rm parallel.tgz                # clean up
	      </PRE
></TD
></TR
></TABLE
>
	    </P
></LI
><LI
><P
>&#13;	      Go to cspkserver.
	      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;ssh cspkserver
	      </PRE
></TD
></TR
></TABLE
>
	    </P
></LI
><LI
><P
>&#13;	      If the command
	      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;set | egrep '(PVM)|(SPK)'
	      </PRE
></TD
></TR
></TABLE
>
	      shows that PVM_ROOT, PVM_ARCH and SPK_SHARE are all defined,
	      <I
CLASS="emphasis"
>skip to the next step</I
>.
	    </P
><P
>&#13;	      Otherwise, edit <TT
CLASS="filename"
>~/.bashrc</TT
> to add the following lines:
	      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;PVM_ROOT=/usr/share/pvm3
SPK_SHARE=/usr/local/spk/share
export PVM_ROOT SPK_SHARE
PVM_ARCH=$($PVM_ROOT/lib/pvmgetarch)
export PVM_ARCH
	      </PRE
></TD
></TR
></TABLE
>
	      then log in again, to activate the new definitions:
	      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;exit
ssh cspkserver
	      </PRE
></TD
></TR
></TABLE
>
	    </P
></LI
><LI
><P
>&#13;	      Make sure that your user name is a member of the <I
CLASS="emphasis"
>spkshare</I
>
	      group.  You can determine this by executing the command
	      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;grep spkshare /etc/group
	      </PRE
></TD
></TR
></TABLE
>
	      and seeing whether or not
	      your user name is in the list at the end of the entry that is displayed.
	      If not, execute the following from a terminal window owned by your ordinary
	      user name:
	      <I
CLASS="emphasis"
>root</I
> user:
	      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;su
gpasswd -a $USER spkshare
exit
	      </PRE
></TD
></TR
></TABLE
>
	    </P
></LI
><LI
><P
>&#13;	      Now you are ready to build and install the prototype.
	      <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;tar xvzf parallel.tgz
cd parallel
make clean
make
make install
	      </PRE
></TD
></TR
></TABLE
>
	    </P
></LI
></OL
>
      </P
></DIV
></DIV
><DIV
CLASS="sect1"
><HR><H2
CLASS="sect1"
><A
NAME="AEN203"
></A
>Running the Parallel SPK Prototype</H2
><P
>&#13;      <P
></P
><OL
TYPE="1"
><LI
><P
>&#13;	    Log into cspkserver as pvm in three separate windows:
	    <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;ssh pvm@cspkserver
ssh pvm@cspkserver
ssh pvm@cspkserver
	    </PRE
></TD
></TR
></TABLE
>
	  </P
></LI
><LI
><P
>&#13;	    Start a real-time display of the log in one of the windows:
	    <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;tail -f $SPK_SHARE/log/spktest/messages
	    </PRE
></TD
></TR
></TABLE
>
	  </P
></LI
><LI
><P
>&#13;	    Start the pvm console in the second window.
	    This will also start the master pvm daemon on cspkserver and
	    add cspkserver to the pvm
	    <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;pvm
	    </PRE
></TD
></TR
></TABLE
>
	  </P
><P
>&#13;	    At the pvm console command prompt, add additional hosts to
	    the pvm:
	    <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;add rose
add azalea
add pasta
add jambutty
	    </PRE
></TD
></TR
></TABLE
>
	  </P
></LI
><LI
><P
>&#13;	    Query the console for a list of hosts:
	    <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;conf
	    </PRE
></TD
></TR
></TABLE
>
	  </P
></LI
><LI
><P
>&#13;	    In the third window, start a job.  The parameter specifies
	    the number of instances (in this case 10 instances).
	    <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;newjob.sh 10
	    </PRE
></TD
></TR
></TABLE
>
	  </P
></LI
><LI
><P
>&#13;	    Query the console for a list of the tasks that are running:
	    <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;ps -a
	    </PRE
></TD
></TR
></TABLE
>
	  </P
></LI
><LI
><P
>&#13;	    Observer messages as they are added to the log.
	  </P
></LI
><LI
><P
>&#13;	    When the first job completes, try another job.  This time
	    delete a node on which several instances of spkind are running
	    <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;delete jambutty
	    </PRE
></TD
></TR
></TABLE
>
	    and see what happens.
	  </P
></LI
><LI
><P
>&#13;	    Run another job.  This time delete the node that spkpop is
	    running on.  (If the node is cspkserver, don't delete it.
	    Wait until the job is done and then start another.)
	    Suppose that spkpop is running on rose.  Delete that node
	    <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;delete rose
	    </PRE
></TD
></TR
></TABLE
>
	    and see what happens.
	  </P
></LI
><LI
><P
>&#13;	    When you are tired of running jobs, halt the pvm by 
	    entering the following at the console prompt:
	    <TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><PRE
CLASS="screen"
>&#13;halt
	    </PRE
></TD
></TR
></TABLE
>
	  </P
></LI
></OL
>
    </P
></DIV
><DIV
CLASS="sect1"
><HR><H2
CLASS="sect1"
><A
NAME="AEN238"
></A
>Copyright Notice</H2
><P
>&#13;      Copyright (c) 2005,  by the University of Washington.
      This material may be distributed only subject to the terms and conditions 
      set forth in the Open Publication License, V1.0 or later
      (the latest version is presently available
      <A
HREF="http://www.opencontent.org/openpub/"
TARGET="_top"
>here</A
>.
    </P
><P
>&#13;    </P
></DIV
></DIV
></BODY
></HTML
>